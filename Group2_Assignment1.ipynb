{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Group 2\n",
    "Authors: Mikayla Russell, Robert Bettaglio, Patrick Kmieciak\n",
    "\n",
    "### Background: Selenium\n",
    "\n",
    "Use Selenium (or pyppeteer) if the page is dynamic, or has login, etc.\n",
    "\n",
    "## Assignment Description\n",
    "\n",
    "Each group selects two websites to crawl:\n",
    "    * needs to have multiple ‘pages’ so there is something to crawl. \n",
    "    * If it has a form to fill out (search form, or login form), that would be good.\n",
    "\n",
    "For our website we chose Indeed.com:\n",
    "    * Find Jobs page\n",
    "    * Company Reviews page\n",
    "\n",
    "### Assignment Steps:\n",
    "\n",
    "1. Crawl the website with Selenium, and save the pages to disk. \n",
    "2. Make sure the crawling is in a loop. If there are 1,000+ pages, then crawling 10 pages is enough.\n",
    "\n",
    "3. In addition to the crawling, keep track of word counts (see below). Depending on the website, this can be by page, or by section of a page. For example, if you are crawling a news website, then keep track of each article. If you are crawling a discussion forum, then this can be either by discussion, or by post.\n",
    "\n",
    "4. Make a few lists holding keywords.\n",
    "\n",
    "    For example (feel free to have your own keywords obviously, can be more than 2 categories):\n",
    "\n",
    "    k1 = [ “tax”, “transfer”, “haven” ]\n",
    "\n",
    "    k2 = [ “bonus”, “compensation”]\n",
    "    \n",
    "    Say you are crawling through SEC correspondence and you want to briefly flag each letter. Every time you find a keyword (tax, panelty, or haven) in a letter, you would increase a counter for k1 (k1_counter), etc. This is to quickly identify a topic.\n",
    "***\n",
    "#### Notes: \n",
    "\n",
    "If the login requires a manual step (like clicking traffic lights, or busses), then it is fine if you do that by hand (click yourself).\n",
    "\n",
    "Don’t use personal login information/passwords for this assignment (or any other assignment), so let’s not crawly your personal email account.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output folder for scraped pages, make sure it exists\n",
    "write_dir = r\"C:\\Users\\RusselM\\Temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\russelm\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\russelm\\anaconda3\\lib\\site-packages (from selenium) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "import time # needed for time.sleep\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new driver (opens Chrome browser window)\n",
    "driver = webdriver.Chrome(r'C:\\Users\\RusselM\\Web Crawling\\Week 1\\class_files_export\\selenium_files\\chromedriver.exe')\n",
    "# go to Indeed Home Page\n",
    "driver.get(\"https://www.indeed.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input element has name=\"q\"\n",
    "elem = driver.find_element_by_id(\"text-input-where\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "# type in search query\n",
    "elem.send_keys(\"X\")\n",
    "\n",
    "#Site automatically inputs \"Henrico County, VA\", does not clear with .clear(), must manually backspace\n",
    "for i in range(0,19):\n",
    "    elem.send_keys(Keys.BACKSPACE)\n",
    "\n",
    "#Enter city to find jobs\n",
    "elem.send_keys(\"Gainesville, FL\")\n",
    "elem.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl \"Find Jobs\" first 10 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Navigate \"Find Jobs\" page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Email Prompt Element...\n"
     ]
    }
   ],
   "source": [
    "# Exception Handling: Delete Email prompt\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "try:\n",
    "    elemEmailPrompt = driver.find_element_by_class_name(\"icl-Icon icl-Icon--sm\")\n",
    "    elemEmailPrompt.click()\n",
    "except NoSuchElementException:\n",
    "    print('No Email Prompt Element...')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com/jobs?q=&l=Gainesville%2C+FL'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentURL = driver.current_url\n",
    "currentURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each page\n",
    "page = 0 \n",
    "for i in range(0, 91, 10):\n",
    "    # First page is automaticly navigated to, define url for pages 2-10\n",
    "    if i == 0:\n",
    "          pageURL = currentURL\n",
    "    else:\n",
    "          pageURL = currentURL + \"&start=\"+ str(i)\n",
    "    # Navigate to specific page, pause for processing\n",
    "    driver.get(pageURL)\n",
    "    driver.implicitly_wait(2)\n",
    "    # grab webpage contents\n",
    "    contents = driver.page_source.encode(\"utf-8\")\n",
    "    # define filename to output location\n",
    "    filename = write_dir + \"\\Page_\" + str(page)\n",
    "    # write to local disk\n",
    "    with open( filename, 'wb') as fd_html:\n",
    "        fd_html.write( contents )\n",
    "    # Define page number for filenames\n",
    "    page = page + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Chrome browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crawl \"Company Reviews\" pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
